{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# 1st Place Solution Training 3D Semantic Segmentation (Stage1)\n","\n","Hi all,\n","\n","I'm very exciting to writing this notebook and the summary of our solution here.\n","\n","This is FULL version of training my final models (stage1), using resnet18d as backbone, unet as decoder and using 128x128x128 as input.\n","\n","NOTE: **You need to run this code locally because the RAM is not enough here.**\n","\n","NOTE2: **It is highly recommended to pre-process the 3D semantic segmentation training data first and save it locally, which can greatly speed up the loading of the data.**\n","\n","My brief summary of winning solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n","\n","* Train Stage1 Notebook: This notebook\n","* Train Stage2 (Type1) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1\n","* Train Stage2 (Type2) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type2\n","* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n","\n","**If you find these notebooks helpful please upvote. Thanks! **"],"metadata":{"id":"yR1uYqSv_9uG"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWuJ-EMFAL_-","executionInfo":{"status":"ok","timestamp":1698132960043,"user_tz":-540,"elapsed":3038,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"cbcf3569-e013-4b86-d487-319c0d9c5469"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 캐글에서 데이터셋 받기 임시\n","import os\n","# %cd /content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/rsna-2022-cervical-spine-fracture-detection\n","os.environ[\"KAGGLE_USERNAME\"] = 'datamollu'\n","os.environ[\"KAGGLE_KEY\"] = \"c5d7d3ad29dc4816fbd98b72e549a26a\"\n","!kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection -p /content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/rsna-2022-cervical-spine-fracture-detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_FigQFyYmDg","executionInfo":{"status":"ok","timestamp":1698132954738,"user_tz":-540,"elapsed":2818876,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"b2df001b-9e33-47a9-e207-2e167b56adc9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading rsna-2022-cervical-spine-fracture-detection.zip to /content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/rsna-2022-cervical-spine-fracture-detection\n","100% 189G/189G [1:51:25<00:00, 29.8MB/s]\n","100% 189G/189G [1:51:25<00:00, 30.4MB/s]\n"]}]},{"cell_type":"code","source":["my_path = \"/content/drive/MyDrive/Colab Notebooks/kaggle/RSNA_2022\""],"metadata":{"id":"-cq94TolBAs4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip -q install monai\n","!pip -q install segmentation-models-pytorch==0.2.1\n","!pip -q install pydicom\n","\n","!pip -q install /content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n","# !pip -q install /content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/pylibjpeg140py3/python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"],"metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:01:58.564856Z","iopub.execute_input":"2023-10-12T11:01:58.565739Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"J1a5goRD_9uJ","executionInfo":{"status":"ok","timestamp":1697788713692,"user_tz":-540,"elapsed":9140,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"55bf307d-a34d-49b8-dbf9-bd5ea7495ec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["DEBUG = False\n","\n","import os\n","import sys\n","sys.path = [\n","    '/content/drive/MyDrive/Colab_Notebooks/data_augmentation/kaggle/RSNA_2022/input/covn3d-same',\n","] + sys.path"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-29T06:00:25.73335Z","iopub.execute_input":"2022-10-29T06:00:25.733795Z","iopub.status.idle":"2022-10-29T06:00:25.741619Z","shell.execute_reply.started":"2022-10-29T06:00:25.733741Z","shell.execute_reply":"2022-10-29T06:00:25.740579Z"},"trusted":true,"id":"RLWjXAhy_9uL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","import albumentations\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","from monai.transforms import Resize\n","import  monai.transforms as transforms\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:25.74295Z","iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z"},"trusted":true,"id":"4yyFB6B-_9uM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"bI7yWEek_9uM"}},{"cell_type":"code","source":["kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","n_folds = 5\n","backbone = 'resnet18d'\n","\n","image_sizes = [128, 128, 128]\n","R = Resize(image_sizes)\n","\n","init_lr = 3e-3\n","batch_size = 4\n","drop_rate = 0.\n","drop_path_rate = 0.\n","loss_weights = [1, 1]\n","p_mixup = 0.1\n","\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","num_workers = 4\n","out_dim = 7\n","\n","n_epochs = 1000\n","\n","log_dir = './logs'\n","model_dir = './models'\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(model_dir, exist_ok=True)"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:29.48482Z","iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z"},"trusted":true,"id":"r_sJwpKf_9uM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transforms_train = transforms.Compose([\n","    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n","    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n","    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n","    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),\n","])\n","\n","transforms_valid = transforms.Compose([\n","])"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.389599Z","iopub.execute_input":"2022-10-29T06:00:34.390438Z","iopub.status.idle":"2022-10-29T06:00:34.402322Z","shell.execute_reply.started":"2022-10-29T06:00:34.3904Z","shell.execute_reply":"2022-10-29T06:00:34.4014Z"},"trusted":true,"id":"bTltzqiY_9uN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DataFrame"],"metadata":{"id":"S9bvd1gy_9uN"}},{"cell_type":"code","source":["df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n","\n","mask_files = os.listdir(f'{data_dir}/segmentations')\n","df_mask = pd.DataFrame({\n","    'mask_file': mask_files,\n","})\n","df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x[:-4])\n","df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir, 'segmentations', x))\n","df = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n","df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n","df['mask_file'].fillna('', inplace=True)\n","\n","df_seg = df.query('mask_file != \"\"').reset_index(drop=True)\n","\n","kf = KFold(5)\n","df_seg['fold'] = -1\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)):\n","    df_seg.loc[valid_idx, 'fold'] = fold\n","\n","df_seg.tail()"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z"},"trusted":true,"id":"uysitsRU_9uN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"iO-Ah3gs_9uO"}},{"cell_type":"code","source":["revert_list = [\n","    '1.2.826.0.1.3680043.1363',\n","    '1.2.826.0.1.3680043.20120',\n","    '1.2.826.0.1.3680043.2243',\n","    '1.2.826.0.1.3680043.24606',\n","    '1.2.826.0.1.3680043.32071'\n","]"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.488061Z","iopub.execute_input":"2022-10-29T06:00:34.488463Z","iopub.status.idle":"2022-10-29T06:00:34.494025Z","shell.execute_reply.started":"2022-10-29T06:00:34.488426Z","shell.execute_reply":"2022-10-29T06:00:34.492953Z"},"trusted":true,"id":"XSssnbhU_9uO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dicom(path):\n","    dicom = pydicom.read_file(path)\n","    data = dicom.pixel_array\n","    data = cv2.resize(data, (image_sizes[0], image_sizes[1]), interpolation = cv2.INTER_LINEAR)\n","    return data\n","\n","\n","def load_dicom_line_par(path):\n","\n","    t_paths = sorted(glob(os.path.join(path, \"*\")),\n","       key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n","\n","    n_scans = len(t_paths)\n","    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_sizes[2])).round().astype(int)\n","    t_paths = [t_paths[i] for i in indices]\n","\n","    images = []\n","    for filename in t_paths:\n","        images.append(load_dicom(filename))\n","    images = np.stack(images, -1)\n","\n","    images = images - np.min(images)\n","    images = images / (np.max(images) + 1e-4)\n","    images = (images * 255).astype(np.uint8)\n","\n","    return images\n","\n","\n","def load_sample(row, has_mask=True):\n","\n","    image = load_dicom_line_par(row.image_folder)\n","    if image.ndim < 4:\n","        image = np.expand_dims(image, 0).repeat(3, 0)  # to 3ch\n","\n","    if has_mask:\n","        mask_org = nib.load(row.mask_file).get_fdata()\n","        shape = mask_org.shape\n","        mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # (d, w, h)\n","        mask = np.zeros((7, shape[0], shape[1], shape[2]))\n","        for cid in range(7):\n","            mask[cid] = (mask_org == (cid+1))\n","        mask = mask.astype(np.uint8) * 255\n","        mask = R(mask).numpy()\n","\n","        return image, mask\n","    else:\n","        return image\n","\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode, transform):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","\n","\n","        ### using local cache\n","#         image_file = os.path.join(data_dir, f'{row.StudyInstanceUID}.npy')\n","#         mask_file = os.path.join(data_dir, f'{row.StudyInstanceUID}_mask.npy')\n","#         image = np.load(image_file).astype(np.float32)\n","#         mask = np.load(mask_file).astype(np.float32)\n","\n","        image, mask = load_sample(row, has_mask=True)\n","\n","        if row.StudyInstanceUID in revert_list:\n","            mask = mask[:, :, :, ::-1]\n","\n","        res = self.transform({'image':image, 'mask':mask})\n","        image = res['image'] / 255.\n","        mask = res['mask']\n","        mask = (mask > 127).astype(np.float32)\n","\n","        image, mask = torch.tensor(image).float(), torch.tensor(mask).float()\n","\n","        return image, mask\n"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z"},"trusted":true,"id":"5f-XWBQw_9uO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rcParams['figure.figsize'] = 20,8\n","\n","df_show = df_seg\n","dataset_show = SEGDataset(df_show, 'train', transform=transforms_train)"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.515883Z","iopub.execute_input":"2022-10-29T06:00:34.516237Z","iopub.status.idle":"2022-10-29T06:00:34.528793Z","shell.execute_reply.started":"2022-10-29T06:00:34.516189Z","shell.execute_reply":"2022-10-29T06:00:34.527571Z"},"trusted":true,"id":"knBw1w6T_9uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(2):\n","    f, axarr = plt.subplots(1,4)\n","    for p in range(4):\n","        idx = i*4+p\n","        img, mask = dataset_show[idx]\n","        img = img[:, :, :, 60]\n","        mask = mask[:, :, :, 60]\n","        mask[0] = mask[0] + mask[3] + mask[6]\n","        mask[1] = mask[1] + mask[4]\n","        mask[2] = mask[2] + mask[5]\n","        mask = mask[:3]\n","        img = img * 0.7 + mask * 0.3\n","        axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:00:34.533491Z","iopub.execute_input":"2022-10-29T06:00:34.533948Z","iopub.status.idle":"2022-10-29T06:02:59.056109Z","shell.execute_reply.started":"2022-10-29T06:00:34.533918Z","shell.execute_reply":"2022-10-29T06:02:59.055183Z"},"trusted":true,"id":"sebSq2Zq_9uP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"9NGfKiI7_9uP"}},{"cell_type":"code","source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=3,\n","            features_only=True,\n","            drop_rate=drop_rate,\n","            drop_path_rate=drop_path_rate,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 3, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply.started":"2022-10-29T06:02:59.05986Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z"},"trusted":true,"id":"7qOG7uJ0_9uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","\n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 3, 128,128,128)).shape"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z"},"trusted":true,"id":"XOtH-_lp_9uP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loss & Metric"],"metadata":{"id":"qLo3l8bV_9uQ"}},{"cell_type":"code","source":["from typing import Any, Dict, Optional\n","\n","\n","def binary_dice_score(\n","    y_pred: torch.Tensor,\n","    y_true: torch.Tensor,\n","    threshold: Optional[float] = None,\n","    nan_score_on_empty=False,\n","    eps: float = 1e-7,\n",") -> float:\n","\n","    if threshold is not None:\n","        y_pred = (y_pred > threshold).to(y_true.dtype)\n","\n","    intersection = torch.sum(y_pred * y_true).item()\n","    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n","\n","    score = (2.0 * intersection) / (cardinality + eps)\n","\n","    has_targets = torch.sum(y_true) > 0\n","    has_predicted = torch.sum(y_pred) > 0\n","\n","    if not has_targets:\n","        if nan_score_on_empty:\n","            score = np.nan\n","        else:\n","            score = float(not has_predicted)\n","    return score\n","\n","\n","def multilabel_dice_score(\n","    y_true: torch.Tensor,\n","    y_pred: torch.Tensor,\n","    threshold=None,\n","    eps=1e-7,\n","    nan_score_on_empty=False,\n","):\n","    ious = []\n","    num_classes = y_pred.size(0)\n","    for class_index in range(num_classes):\n","        iou = binary_dice_score(\n","            y_pred=y_pred[class_index],\n","            y_true=y_true[class_index],\n","            threshold=threshold,\n","            nan_score_on_empty=nan_score_on_empty,\n","            eps=eps,\n","        )\n","        ious.append(iou)\n","\n","    return ious\n","\n","\n","def dice_loss(input, target):\n","    input = torch.sigmoid(input)\n","    smooth = 1.0\n","    iflat = input.view(-1)\n","    tflat = target.view(-1)\n","    intersection = (iflat * tflat).sum()\n","    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n","\n","\n","def bce_dice(input, target, loss_weights=loss_weights):\n","    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n","    loss2 = loss_weights[1] * dice_loss(input, target)\n","    return (loss1 + loss2) / sum(loss_weights)\n","\n","criterion = bce_dice"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:13.72458Z","iopub.execute_input":"2022-10-29T06:03:13.724951Z","iopub.status.idle":"2022-10-29T06:03:13.73657Z","shell.execute_reply.started":"2022-10-29T06:03:13.72492Z","shell.execute_reply":"2022-10-29T06:03:13.7354Z"},"trusted":true,"id":"iJK89ETr_9uQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train & Valid func"],"metadata":{"id":"bud7jYzb_9uQ"}},{"cell_type":"code","source":["def mixup(input, truth, clip=[0, 1]):\n","    indices = torch.randperm(input.size(0))\n","    shuffled_input = input[indices]\n","    shuffled_labels = truth[indices]\n","\n","    lam = np.random.uniform(clip[0], clip[1])\n","    input = input * lam + shuffled_input * (1 - lam)\n","    return input, truth, shuffled_labels, lam\n","\n","\n","def train_func(model, loader_train, optimizer, scaler=None):\n","    model.train()\n","    train_loss = []\n","    bar = tqdm(loader_train)\n","    for images, gt_masks in bar:\n","        optimizer.zero_grad()\n","        images = images.cuda()\n","        gt_masks = gt_masks.cuda()\n","\n","        do_mixup = False\n","        if random.random() < p_mixup:\n","            do_mixup = True\n","            images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n","\n","        with amp.autocast():\n","            logits = model(images)\n","            loss = criterion(logits, gt_masks)\n","            if do_mixup:\n","                loss2 = criterion(logits, gt_masks_sfl)\n","                loss = loss * lam  + loss2 * (1 - lam)\n","\n","        train_loss.append(loss.item())\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n","\n","    return np.mean(train_loss)\n","\n","\n","def valid_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    ths = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n","    batch_metrics = [[]] * 7\n","    bar = tqdm(loader_valid)\n","    with torch.no_grad():\n","        for images, gt_masks in bar:\n","            images = images.cuda()\n","            gt_masks = gt_masks.cuda()\n","\n","            logits = model(images)\n","            loss = criterion(logits, gt_masks)\n","            valid_loss.append(loss.item())\n","            for thi, th in enumerate(ths):\n","                pred = (logits.sigmoid() > th).float().detach()\n","                for i in range(logits.shape[0]):\n","                    tmp = multilabel_dice_score(\n","                        y_pred=logits[i].sigmoid().cpu(),\n","                        y_true=gt_masks[i].cpu(),\n","                        threshold=0.5,\n","                    )\n","                    batch_metrics[thi].extend(tmp)\n","            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n","\n","    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n","    print('best th:', ths[np.argmax(metrics)], 'best dc:', np.max(metrics))\n","\n","    return np.mean(valid_loss), np.max(metrics)\n"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z"},"trusted":true,"id":"JJw9Ety8_9uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rcParams['figure.figsize'] = 20, 2\n","optimizer = optim.AdamW(m.parameters(), lr=init_lr)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000)\n","lrs = []\n","for epoch in range(1, 1000+1):\n","    scheduler_cosine.step(epoch-1)\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","plt.plot(range(len(lrs)), lrs)"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:13.757188Z","iopub.execute_input":"2022-10-29T06:03:13.757646Z","iopub.status.idle":"2022-10-29T06:03:13.977069Z","shell.execute_reply.started":"2022-10-29T06:03:13.75761Z","shell.execute_reply":"2022-10-29T06:03:13.976109Z"},"trusted":true,"id":"LmjkBaPJ_9uQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"Laa_-KuC_9uQ"}},{"cell_type":"code","source":["def run(fold):\n","\n","    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","\n","    train_ = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n","    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n","    dataset_train = SEGDataset(train_, 'train', transform=transforms_train)\n","    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid)\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","    model = model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n","    scaler = torch.cuda.amp.GradScaler()\n","    from_epoch = 0\n","    metric_best = 0.\n","    loss_min = np.inf\n","\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs)\n","\n","    print(len(dataset_train), len(dataset_valid))\n","\n","    for epoch in range(1, n_epochs+1):\n","        scheduler_cosine.step(epoch-1)\n","\n","        print(time.ctime(), 'Epoch:', epoch)\n","\n","        train_loss = train_func(model, loader_train, optimizer, scaler)\n","        valid_loss, metric = valid_func(model, loader_valid)\n","\n","        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n","        print(content)\n","        with open(log_file, 'a') as appender:\n","            appender.write(content + '\\n')\n","\n","        if metric > metric_best:\n","            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n","            torch.save(model.state_dict(), model_file)\n","            metric_best = metric\n","\n","        # Save Last\n","        if not DEBUG:\n","            torch.save(\n","                {\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n","                    'score_best': metric_best,\n","                },\n","                model_file.replace('_best', '_last')\n","            )\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z"},"trusted":true,"id":"i29G8LIC_9uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run(0)\n","run(1)\n","run(2)\n","run(3)\n","run(4)"],"metadata":{"execution":{"iopub.status.busy":"2022-10-29T06:03:33.005515Z","iopub.execute_input":"2022-10-29T06:03:33.006143Z"},"trusted":true,"id":"cRkK8jP9_9uR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vjGcTX5L_9uR"},"execution_count":null,"outputs":[]}]}