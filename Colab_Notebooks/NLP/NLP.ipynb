{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMvkaUXbp6G6iFmvJ+W3vaz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 텍스트 데이터 전처리\n","1. 데이터 표준화\n","- 대문자나 발음 표현 기호 같은거 다 무시하고 동일하게 소문자로 통일\n","- 쉼표나 콤마 등 스톱워드(의미가 없는 문자들) 삭제\n","- 머신 러닝에서 드물게 문법 다 무시하고 기본형으로 바꿔버리는 어간 추출이란 기법도 있다( 물론 그만큼 원본에 대한 데이터는 사라지는 게 맞다)\n","2. 텍스트 토큰화\n","- 시퀀스 모델과 BoW모델로 나뉜다\n","- BoW모델의 경우 문장의 순서에 대한 정보는 거의 없지만, 2-gram, 3-gram등 순서정보가 눈꼽만큼 들어가 있는 토큰화 방법도 있다\n","\n","\n","\n"," 토큰화를 진행할 때 각 토큰을 수치로 인코딩하여 벡터화 시켜야 하는데, 이를 지금부터 구현해보도록 하자.\n"," **주의할 점**\n"," - 어휘사전에 모든 토큰이 기록되어 있지 않기 때문에 예외 단어를 위한 인덱스를 만들어 둔다(보통은 1을 사용한다 -> 1은 어휘사전에 없는 모든 단어에 대응함)\n"," - 이러한 예외처리 토큰을 OOV(out of vocabulary)라고 부른다\n"," - 0번쨰 인덱스는 일반적으로 무시할 수 있는 토큰을 매핑하기 위해 사용한다."],"metadata":{"id":"Sxl9z9kwC6AI"}},{"cell_type":"code","source":["from tensorflow.keras.layers import TextVectorization\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\"\n",")"],"metadata":{"id":"byxo9uAXIu9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms\"\n","]\n","text_vectorization.adapt(dataset) # 말뭉치로 어휘사전 인덱싱 가능"],"metadata":{"id":"je2QF30SLZRw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_vectorization.get_vocabulary() # 저장된 어휘사전 열람"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nk4jeQMKO0td","executionInfo":{"status":"ok","timestamp":1695017237035,"user_tz":-540,"elapsed":6,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"e42dd392-3137-4419-eab0-02d0dbd72cc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'erase',\n"," 'write',\n"," 'then',\n"," 'rewrite',\n"," 'poppy',\n"," 'i',\n"," 'blooms',\n"," 'and',\n"," 'again',\n"," 'a']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["vocabulary = text_vectorization.get_vocabulary()\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = text_vectorization(test_sentence)\n","print(\"encoded sentence : \", encoded_sentence)\n","inverse_vocab = dict(enumerate(vocabulary))\n","decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n","print(\"decoded sentence : \", decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a-r_euKPLks","executionInfo":{"status":"ok","timestamp":1695017237406,"user_tz":-540,"elapsed":374,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"5416e586-2b9f-4317-83ea-d17f51881dc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encoded sentence :  tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n","decoded sentence :  i write rewrite and [UNK] rewrite again\n"]}]},{"cell_type":"markdown","source":["# IMDB 영화 리뷰 데이터 준비하기"],"metadata":{"id":"JBtXHACoN_qD"}},{"cell_type":"code","source":["!curl -0 https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz --output aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wJzLacVSgpI","executionInfo":{"status":"ok","timestamp":1695191147308,"user_tz":-540,"elapsed":20143,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"30082f85-b9a9-43c8-ec11-e1dbb75316ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  9224k      0  0:00:08  0:00:08 --:--:-- 17.2M\n"]}]},{"cell_type":"code","source":["!rm -r aclImdb/train/unsup\n","!cat aclImdb/train/pos/4077_10.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7j59J7VSuDJ","executionInfo":{"status":"ok","timestamp":1695191148639,"user_tz":-540,"elapsed":1333,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"47a95c74-a1e0-4f2e-8373-c18743de8fd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"]}]},{"cell_type":"code","source":["import os, pathlib, shutil, random\n","\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","  os.makedirs(val_dir / category)\n","  files = os.listdir(train_dir / category)\n","  random.Random(1337).shuffle(files)\n","  num_val_samples = int(0.2 * len(files))\n","  val_files = files[-num_val_samples:]\n","  for fname in val_files:\n","    shutil.move(train_dir / category / fname, val_dir / category / fname)"],"metadata":{"id":"kt0RGLgiUGYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","batch_size = 32\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    train_dir, batch_size = batch_size\n",")\n","\n","val_ds = keras.utils.text_dataset_from_directory(\n","    val_dir, batch_size = batch_size\n",")\n","\n","test_ds = keras.utils.text_dataset_from_directory(\n","    base_dir / \"test\", batch_size = batch_size\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvOT99MOcVZY","executionInfo":{"status":"ok","timestamp":1695191158575,"user_tz":-540,"elapsed":9938,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"fbebb515-b7e3-43f8-8fdb-c9adc8cba245"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["for inputs, targets in train_ds:\n","  print(\"inputs.shape : \", inputs.shape)\n","  print(\"inputs.dtype : \", inputs.dtype)\n","  print(\"targets.shape : \", targets.shape)\n","  print(\"targets.dtype : \", targets.dtype)\n","  print(\"input[0] : \", inputs[0])\n","  print(\"target[0] : \", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTACNMT0c7KG","executionInfo":{"status":"ok","timestamp":1695191158576,"user_tz":-540,"elapsed":9,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"6f61b405-e1f4-49bd-e25d-01be8aabb201"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape :  (32,)\n","inputs.dtype :  <dtype: 'string'>\n","targets.shape :  (32,)\n","targets.dtype :  <dtype: 'int32'>\n","input[0] :  tf.Tensor(b'[***POSSIBLE SPOILERS***] This movie\\'s reputation precedes it, so it was with anticipation that I sat down to watch it in letterbox on TCM. What a major disappointment.<br /><br />The cast is superb and the production values are first-rate, but the characters are without depth, the plot is thin, and the whole thing goes on too long. For a movie that deals with alcoholism, family divisions, unfaithfulness, gambling, and sexual repression, the movie is curiously flat, prosaic, lifeless, and cliche-ridden. One example is the portrayal of Frank Hirsch\\'s unfaithfuness: his rather heavy-handed request to his wife to \"go upstairs and relax a bit\" followed by her predictable pleading of a headache, leads - even more predictably - to his evening liaison with his secretary (\"hey Nancy, I\\'ve got the blues tonight. Let\\'s go for a drive\"), all according to well-worn formula. We don\\'t feel these are real people, but cardboard cutouts acting in a marionette play. Also, the source of the obvious friction between Frank and Dave Hirsch is never really explored or explained. Dave\\'s infatuation with the on-again/off-again Gwen is inexplicable in light of her fatuous inability to defecate or get off the pot. His subsequent marriage of desperation to the Shirley Maclaine/Ginny character is, from the moment of its being presented to this viewer, anyway, obviously doomed to fail, and it was clear - by the conventions of this type of soap opera - that it could only be resolved by someone being killed. The moment the jealous lover started running around with the gun I started a bet with myself as to who - Dave or Ginny - would get killed. The whole thing was phony with a capital \\'P\\'. <br /><br />Having said that, Maclaine\\'s performance and that of Dean Martin are the standouts here. But on the whole I find the movie\\'s interest to be purely that of a period piece of Hollywood history.', shape=(), dtype=string)\n","target[0] :  tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["# BoW 방식의 처리\n","- 단어사전에 등록된 단어 갯수를 20000개로 해서 위의 string을 shape 20000, 의 one_hot_vector로 만들어 보자!!!\n"],"metadata":{"id":"J3jPYJaLdbdl"}},{"cell_type":"code","source":["text_vectorization = TextVectorization(\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\" # 멀티_핫_이진벡터로 출력\n",")\n","\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(text_only_train_ds)\n","binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"],"metadata":{"id":"ulE6JULgd940"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in binary_1gram_train_ds:\n","  print(\"inputs.shape : \", inputs.shape)\n","  print(\"inputs.dtype : \", inputs.dtype)\n","  print(\"targets.shape : \", targets.shape)\n","  print(\"targets.dtype : \", targets.dtype)\n","  print(\"input[0] : \", inputs[0])\n","  print(\"target[0] : \", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4hv622Sefbh","executionInfo":{"status":"ok","timestamp":1695191182527,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"2efbdfe4-45fb-41ea-b343-092b37fc5c91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape :  (32, 20000)\n","inputs.dtype :  <dtype: 'float32'>\n","targets.shape :  (32,)\n","targets.dtype :  <dtype: 'int32'>\n","input[0] :  tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","target[0] :  tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["# 2-gram bow 방식 사용\n","- 위에서는 unigram 방식으로 BoW 인코딩을 진행하였다, 2-gram 방식의 인코딩도 진행해보자\n","\n"],"metadata":{"id":"OU6KZM8rekYS"}},{"cell_type":"code","source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\" # 멀티_핫_이진벡터로 출력\n",")"],"metadata":{"id":"_eoccON8ewgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_vectorization.adapt(text_only_train_ds)\n","binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"],"metadata":{"id":"5A0YR2RGe9w5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in binary_2gram_train_ds:\n","  print(\"inputs.shape : \", inputs.shape)\n","  print(\"inputs.dtype : \", inputs.dtype)\n","  print(\"targets.shape : \", targets.shape)\n","  print(\"targets.dtype : \", targets.dtype)\n","  print(\"input[0] : \", inputs[0])\n","  print(\"target[0] : \", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyPdO0jTfFOk","executionInfo":{"status":"ok","timestamp":1695191195667,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"ed2c356d-efb8-4923-8231-adb6c17f8a20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape :  (32, 20000)\n","inputs.dtype :  <dtype: 'float32'>\n","targets.shape :  (32,)\n","targets.dtype :  <dtype: 'int32'>\n","input[0] :  tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","target[0] :  tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["# TF-IDF 인코딩을 사용한 바이그램\n","- 개별단어나 N-그램의 등장 횟수를 카운트한 정보를 추가하기 위한 기법이다.\n","- TF-IDF 인코딩을 활용할 때는 단어마다 카운트를 세어 많이 등장하는 단어에 대해 가중치를 부여한다.\n","- 그러나, a 나 the같은 의미는 없지만 많이 쓰이는 단어에 대해서만 너무 카운트가 커져버리는 단점이 있다.\n","- 그렇기에 특정 단어에 대해 현재 문서에서 많이 쓰일수록, 다른 문서에서 적게 쓰일수록 해당 단어의 가중치를 높게 잡는 방식을 활용하여 각 단어의 Count를 정규화해준다.\n","- 각 단어에 대해 Count = (문서에서 쓰인 횟수) / log(전체문서에서 쓰인 횟수)\n","\n"],"metadata":{"id":"3mSxopCTfM67"}},{"cell_type":"code","source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"tf_idf\" # tf-idf 사용\n",")"],"metadata":{"id":"sXCfH-0EgOls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_vectorization.adapt(text_only_train_ds)\n","tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n","tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"],"metadata":{"id":"0WU7h575gVQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in tfidf_2gram_train_ds:\n","  print(\"inputs.shape : \", inputs.shape)\n","  print(\"inputs.dtype : \", inputs.dtype)\n","  print(\"targets.shape : \", targets.shape)\n","  print(\"targets.dtype : \", targets.dtype)\n","  print(\"input[0] : \", inputs[0])\n","  print(\"target[0] : \", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5X6J8uLgcKG","executionInfo":{"status":"ok","timestamp":1695191216238,"user_tz":-540,"elapsed":6,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"1c9583c7-3539-458f-88de-6656cd207672"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape :  (32, 20000)\n","inputs.dtype :  <dtype: 'float32'>\n","targets.shape :  (32,)\n","targets.dtype :  <dtype: 'int32'>\n","input[0] :  tf.Tensor(\n","[986.4775     14.647509    1.4225553 ...   0.          0.\n","   0.       ], shape=(20000,), dtype=float32)\n","target[0] :  tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["# 시퀀스 모델\n","- 지금까지는 단어의 순서에 관한 정보를 거의 신경 안쓰도록 하는 data를 준비해봤다.(따로 fully connected layer로 학습은 안함..)\n","- 지금부터는 RNN을 활용하여 단어의 순서까지 고려하는 모델을 구성해보겠따\n","- 케라스 책에서는 각 단어를 단순 one_hot_vector로 나타내는 거부터 하긴 하는데 그냥 단어 임베딩을 바로 활용해 보도록 하자\n","\n","\n","# 단어 임베딩\n","- 각 단어를 임의의 벡터로 나타내는 행렬을 학습한다\n","- 학습과정에서 woman이나 girl같은 유사한 단어들은 코사인 유사도나 L2거리가 작고, woman의 단어 벡터와 king의 단어벡터를 합치면 queen의 단어벡터가 나오는 등 여러 훌륭한 특징들을 도출해 낼 수 있다.\n","\n","1. 직접 이러한 가중치행렬을 학습시키거나\n","2. 사전 훈련된 단어 임베딩을 활용하는 방법이 있다."],"metadata":{"id":"CITUsDbZgfKq"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","max_tokens = 20000\n","embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256) # 각 문장을 max_tokens 길이의 문장으로, 단어 벡터의 차원을 256으로 설정\n","# ( batch_size, sequence_length ) 를 입력으로 받고, ( batch_size, sequence_length, output_dim )을 출력으로 내놓음"],"metadata":{"id":"Ez5ucGAah2q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary() # 임베딩 층을 활용하는 양방향 LSTM 예시 훈련은 안할거임.. 너무 오래걸림"],"metadata":{"id":"4Nop5IdXiobc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 패딩과 마스킹 이해하기\n","\n","- TextVectorizaion이나 Embedding 층을 활용하여 단어를 인코딩 할 때 각 문장은 max_tokens 갯수로 짤리게 된다\n","- 만약 최대 문장 길이 600까지만 본다면 그보다 짧은 문장은 0으로 패딩이 이루어지고 그보다 긴 문장은 짤린다\n","- 그런데 짧은 문장의 경우 끝부분이 0으로 가득차게 되면서 최종 예측에 큰 영향을 미치기 때문에 이를 건너뛸 방법이 필요한데 이것이 마스킹이다\n"],"metadata":{"id":"NC6XjHd8jQea"}},{"cell_type":"code","source":["embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n","some_input = [\n","    [4, 3, 2, 1, 0, 0, 0],\n","    [5, 4, 3, 2, 1, 0, 0],\n","    [2, 1, 0, 0, 0, 0, 0]\n","]\n","mask = embedding_layer.compute_mask(some_input)\n","mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MP3aegGYkvcN","executionInfo":{"status":"ok","timestamp":1695191216239,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"6c1c7cb8-8f05-45c7-d60c-4437707c3c0a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n","array([[ True,  True,  True,  True, False, False, False],\n","       [ True,  True,  True,  True,  True, False, False],\n","       [ True,  True, False, False, False, False, False]])>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# 사전 훈련된 임베딩 사용하기\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgnJrg5KlJvM","executionInfo":{"status":"ok","timestamp":1695191266676,"user_tz":-540,"elapsed":50441,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"ae248df6-e68a-4bb3-94dd-fef3d6d59aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-09-20 06:26:23--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-09-20 06:26:24--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-09-20 06:26:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip         30%[=====>              ] 252.12M  5.02MB/s    eta 1m 49s ^C\n","[glove.6B.zip]\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of glove.6B.zip or\n","        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","path_to_glove_file = \"glove.6B.100d.txt\"\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","  for line in f:\n","    word, coefs = line.split(maxsplit=1)\n","    coefs = np.fromstring(coefs, 'f', sep=\" \")\n","    embeddings_index[word] = coefs\n","\n","print(f\"단어 벡터 갯수 : {len(embeddings_index)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wg9GQlUrpoKn","executionInfo":{"status":"ok","timestamp":1695017479848,"user_tz":-540,"elapsed":5449,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"e5dcf28d-b4c2-4a9c-fb27-c78f7453038f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 벡터 갯수 : 400000\n"]}]},{"cell_type":"code","source":["embedding_dim = 100\n","\n","vocabulary = text_vectorization.get_vocabulary()\n","word_index = dict(zip(vocabulary, range(len(vocabulary))))\n","\n","embedding_matrix = np.zeros((max_tokens, embedding_dim))\n","for word, i in word_index.items():\n","  if i < max_tokens:\n","    embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"metadata":{"id":"nsvlD9ATqMBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_layer = layers.Embedding(\n","    max_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n","    mask_zero=True\n",")"],"metadata":{"id":"aKpToY9rrHsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = embedding_layer(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlrMW3f4sBPh","executionInfo":{"status":"ok","timestamp":1695017481266,"user_tz":-540,"elapsed":1423,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"16918c08-b813-4964-c986-3a6f79fd5992"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_3 (Embedding)     (None, None, 100)         2000000   \n","                                                                 \n"," bidirectional_1 (Bidirecti  (None, 64)                34048     \n"," onal)                                                           \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2034113 (7.76 MB)\n","Trainable params: 34113 (133.25 KB)\n","Non-trainable params: 2000000 (7.63 MB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["max_length = 600\n","max_tokens = 20000\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=max_length\n",")\n","text_vectorization.adapt(text_only_train_ds)\n","int_train_ds = train_ds.map(\n","    lambda x, y : (text_vectorization(x), y), num_parallel_calls=4\n",")\n","\n","int_val_ds = val_ds.map(\n","    lambda x, y : (text_vectorization(x), y), num_parallel_calls=4\n",")\n","\n","int_test_ds = test_ds.map(\n","    lambda x, y : (text_vectorization(x), y), num_parallel_calls=4\n",")"],"metadata":{"id":"-OcwfdXRsraw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in int_train_ds:\n","  print(\"inputs.shape : \", inputs.shape)\n","  print(\"inputs.dtype : \", inputs.dtype)\n","  print(\"targets.shape : \", targets.shape)\n","  print(\"targets.dtype : \", targets.dtype)\n","  print(\"input[0] : \", inputs[0])\n","  print(\"target[0] : \", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ttqsoM5xp00","executionInfo":{"status":"ok","timestamp":1695017484218,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"612ae16d-ca84-42a4-b4dd-950f05d8578f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape :  (32, 600)\n","inputs.dtype :  <dtype: 'int64'>\n","targets.shape :  (32,)\n","targets.dtype :  <dtype: 'int32'>\n","input[0] :  tf.Tensor(\n","[   10   281    11   522  4112     4    10    26     6   129    30     2\n","  1793   195    20    92    17     1   192   121   108   773     1     2\n","    20     7   384   283  3105    46    45    23    24     3   842   327\n","    11    19 10152     5  1329     4   830    47     5     2   647     5\n","     2     1    24     2   115   121  2408     6    20    52   927   276\n","  9730     1  5247  1261     1     6    65  1819  1378     1    13   914\n","     1  2227   364    16  1258  2557    78   382    23     1   158     2\n","   195     1    30  5851  3661  2214   417    11    20  2202    37   377\n","     6  1405    10   393   936    17    53    72    12    54    60  3254\n","     7    12     7    14   100   352     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n","target[0] :  tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"glove_embeddings_seq_model.x\", save_best_only=True)\n","]\n","\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfQLxWtruqoL","outputId":"8cad0d64-bea4-4b23-92b5-40d348113682","executionInfo":{"status":"ok","timestamp":1695020083507,"user_tz":-540,"elapsed":2599292,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 256s 399ms/step - loss: 0.6788 - accuracy: 0.5641 - val_loss: 0.6457 - val_accuracy: 0.6194\n","Epoch 2/10\n","625/625 [==============================] - 261s 418ms/step - loss: 0.6397 - accuracy: 0.6342 - val_loss: 0.6189 - val_accuracy: 0.6576\n","Epoch 3/10\n","625/625 [==============================] - 258s 412ms/step - loss: 0.6116 - accuracy: 0.6667 - val_loss: 0.5990 - val_accuracy: 0.6728\n","Epoch 4/10\n","625/625 [==============================] - 260s 416ms/step - loss: 0.5843 - accuracy: 0.6905 - val_loss: 0.5761 - val_accuracy: 0.6950\n","Epoch 5/10\n","625/625 [==============================] - 258s 413ms/step - loss: 0.5608 - accuracy: 0.7096 - val_loss: 0.5523 - val_accuracy: 0.7138\n","Epoch 6/10\n","625/625 [==============================] - 257s 412ms/step - loss: 0.5364 - accuracy: 0.7290 - val_loss: 0.5430 - val_accuracy: 0.7182\n","Epoch 7/10\n","625/625 [==============================] - 260s 415ms/step - loss: 0.5166 - accuracy: 0.7447 - val_loss: 0.5317 - val_accuracy: 0.7300\n","Epoch 8/10\n","625/625 [==============================] - 236s 377ms/step - loss: 0.4975 - accuracy: 0.7586 - val_loss: 0.5419 - val_accuracy: 0.7196\n","Epoch 9/10\n","625/625 [==============================] - 254s 407ms/step - loss: 0.4744 - accuracy: 0.7753 - val_loss: 0.5000 - val_accuracy: 0.7524\n","Epoch 10/10\n","625/625 [==============================] - 258s 412ms/step - loss: 0.4586 - accuracy: 0.7810 - val_loss: 0.4995 - val_accuracy: 0.7486\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7bb3623dba00>"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["# 셀프 어텐션 이해하기\n","- 트랜스포머 모델을 살펴보기 전에 셀프 어텐션 메커니즘에 대해 이해해보자"],"metadata":{"id":"Yr78j_1StZig"}},{"cell_type":"code","source":["def self_attention(input_sequence):\n","  output = np.zeros_like(input_sequence)\n","  for i, pivot_vector in enumerate(input_sequence): # 각 단어 벡터 pivot_vector에 대해서\n","    scores = np.zeros(shape = (len(input_sequence),))\n","    for j, vector in enumerate(input_sequence): # 그 단어와 다른 단어의 유사도를 내적으로 계산한다\n","      scores[j] = np.dot(pivot_vector, vector.T)\n","    scores /= np.sqrt(input_sequence.shape[1]) # 단어 공간 차원 크기의 제곱근으로 정규화\n","    scores = np.softmax(scores) # 소프트맥스\n","    new_pivot_representation = np.zeros_like(pivot_vector)\n","    for j, vector in enumerate(input_sequence): # 해당 pivot_vector의 새로운 표현으로 각 벡터를 스코어배 해서 선형결합한 벡터를 얻어냄\n","      new_pivot_representation += vector * scores[j]\n","    output[i] = new_pivot_representation # 이렇게 얻어낸 새로운 표현으로 각 단어 벡터를 인코딩함\n","  return output"],"metadata":{"id":"3KDmvo6WzJud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 트랜스포머로 seq2seq 학습 해보기\n"],"metadata":{"id":"CY_O4b4D7EWZ"}},{"cell_type":"code","source":["!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","!unzip -q spa-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPMJDMNTRnl2","executionInfo":{"status":"ok","timestamp":1695279574350,"user_tz":-540,"elapsed":86933,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"9dd3e6ee-5ffb-4075-8e5b-ac97431ec38d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-09-21 06:57:33--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.175.207, 74.125.24.207, 142.250.4.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.175.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2638744 (2.5M) [application/zip]\n","Saving to: ‘spa-eng.zip.1’\n","\n","spa-eng.zip.1       100%[===================>]   2.52M  2.11MB/s    in 1.2s    \n","\n","2023-09-21 06:57:35 (2.11 MB/s) - ‘spa-eng.zip.1’ saved [2638744/2638744]\n","\n","replace spa-eng/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["text_file = \"spa-eng/spa.txt\"\n","with open(text_file) as f:\n","  lines = f.read().split('\\n')[:-1]\n","text_pairs = []\n","for line in lines:\n","  english, spanish = line.split(\"\\t\")\n","  spanish = \"[start]\" + spanish + \"[end]\"\n","  text_pairs.append((english, spanish))"],"metadata":{"id":"ccs3cp3gSTTH","executionInfo":{"status":"ok","timestamp":1695279574351,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import random\n","print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOBdJmLAT_8V","executionInfo":{"status":"ok","timestamp":1695279574351,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"8faaabca-af14-48c9-c30c-421637de338b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["('Tom is really a good worker.', '[start]Tomás es realmente un buen trabajador.[end]')\n"]}]},{"cell_type":"code","source":["import random\n","\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]"],"metadata":{"id":"2kxh4TbhUDKb","executionInfo":{"status":"ok","timestamp":1695279574351,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 영어와 스페인어 텍스트 쌍 벡터화"],"metadata":{"id":"oQk2V25fU-1H"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import string\n","import re\n","\n","strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","  lowercase = tf.strings.lower(input_string)\n","  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","vocab_size = 15000\n","sequence_length = 20\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length\n",")\n","\n","target_vectorization = layers.TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length = (sequence_length+1),\n","    standardize = custom_standardization,\n",")\n","\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"],"metadata":{"id":"aQVWEPZAVHJq","executionInfo":{"status":"ok","timestamp":1695279595487,"user_tz":-540,"elapsed":21139,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","\n","def format_dataset(eng, spa):\n","  eng = source_vectorization(eng)\n","  spa = target_vectorization(spa)\n","  return ({\n","      \"english\" : eng,\n","      \"spanish\" : spa[:, :-1] # 입력은 [start]어쩌구 저쩌구\n","  }, spa[:, 1:])  #출력은 저쩌구 저쩌구[end]\n","\n","def make_dataset(pairs):\n","  eng_texts, spa_texts = zip(*pairs)\n","  eng_texts = list(eng_texts)\n","  spa_texts = list(spa_texts)\n","  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","  return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"UCUimolyYQHC","executionInfo":{"status":"ok","timestamp":1695279596198,"user_tz":-540,"elapsed":713,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","  print(f\"inputs['english'].shape : {inputs['english'].shape}\")\n","  print(f\"inputs['spanish'].shape : {inputs['spanish'].shape}\")\n","  print(f\"targets.shape : {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOORPKEacmpr","executionInfo":{"status":"ok","timestamp":1695279596984,"user_tz":-540,"elapsed":788,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"a8c4eec1-80a8-4df6-8793-2970c56b7d42"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape : (64, 20)\n","inputs['spanish'].shape : (64, 20)\n","targets.shape : (64, 20)\n"]}]},{"cell_type":"markdown","source":["# 트랜스포머 encoding layer"],"metadata":{"id":"55Tm3dHcd2vG"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","    super().__init__(**kwargs)\n","    self.embed_dim = embed_dim\n","    self.dense_dim = dense_dim\n","    self.num_heads = num_heads\n","    self.attention = layers.MultiHeadAttention(\n","        num_heads=num_heads, key_dim=embed_dim\n","    )\n","    self.dense_proj = keras.Sequential(\n","        [layers.Dense(dense_dim, activation=\"relu\"),\n","         layers.Dense(embed_dim)]\n","    )\n","    self.layernorm_1 = layers.LayerNormalization()\n","    self.layernorm_2 = layers.LayerNormalization()\n","\n","  def call(self, inputs, mask=None):\n","    if mask is not None:\n","      mask = mask[:, tf.newaxis, :] # num_heads에 대한 차원이 추가되기 때문에 요렇게 해줌\n","    attention_output = self.attention(\n","        inputs, inputs, attention_mask=mask # 인코딩 MultiHeadAttention Q, K, V 모두 source sequence\n","    )\n","    proj_input = self.layernorm_1(inputs + attention_output)\n","    proj_output = self.dense_proj(proj_input)\n","    return self.layernorm_2(proj_input + proj_output)\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","        \"embed_dim\" : self.embed_dim,\n","        \"num_heads\" : self.num_heads,\n","        \"dense_dim\" : self.dense_dim\n","    })\n","    return config\n","# 사용자 정의층을 지정할 때 위와같이 get_config 함수를 지정해서 직렬화 해 주어야 한다.\n","# keras.models.load_model 사용시에도 custom_objects={\"층 이름\" : 층이름} 식으로 사용자 정의 클래스를 명시해야 된다."],"metadata":{"id":"5sMbtZoLJ6d2","executionInfo":{"status":"ok","timestamp":1695279596985,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    self.token_embeddings = layers.Embedding(\n","        input_dim=input_dim, output_dim=output_dim\n","    )\n","    self.position_embeddings = layers.Embedding(\n","        input_dim=sequence_length, output_dim=output_dim\n","    )\n","    self.sequence_length = sequence_length\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","\n","  def call(self, inputs):\n","    length = tf.shape(inputs)[-1]\n","    positions = tf.range(start=0, limit=length, delta=1)\n","    embedded_tokens = self.token_embeddings(inputs)\n","    embedded_positions = self.position_embeddings(positions)\n","    return embedded_tokens + embedded_positions\n","\n","  def compute_mask(self, inputs, mask=None):\n","    return tf.math.not_equal(inputs, 0) # 정수 시퀀스인 inputs 중 0인 값에 대하여 False로 마스킹\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","        \"output_dim\" : self.output_dim,\n","        \"input_dim\" : self.input_dim,\n","        \"sequence_length\" : self.sequence_length\n","    })\n","    return config"],"metadata":{"id":"utsOMpdxM_bQ","executionInfo":{"status":"ok","timestamp":1695279761066,"user_tz":-540,"elapsed":705,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","    super().__init__(**kwargs)\n","    self.embed_dim = embed_dim\n","    self.dense_dim = dense_dim\n","    self.num_heads = num_heads\n","    self.attention_1 = layers.MultiHeadAttention(\n","        num_heads = num_heads, key_dim = embed_dim\n","    )\n","    self.attention_2 = layers.MultiHeadAttention(\n","        num_heads = num_heads, key_dim = embed_dim\n","    )\n","    self.dense_proj = keras.Sequential([\n","        layers.Dense(dense_dim, activation=\"relu\"),\n","        layers.Dense(embed_dim)\n","    ])\n","    self.layernorm_1 = layers.LayerNormalization()\n","    self.layernorm_2 = layers.LayerNormalization()\n","    self.layernorm_3 = layers.LayerNormalization()\n","\n","  def get_causal_attention_mask(self, inputs):\n","    input_shape = tf.shape(inputs)\n","    batch_size, sequence_length = input_shape[0], input_shape[1]\n","    i = tf.range(sequence_length)[:, tf.newaxis]\n","    j = tf.range(sequence_length)\n","    mask = tf.cast(i >= j, dtype=\"int32\")\n","    mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n","    mult = tf.concat(\n","        [tf.expand_dims(batch_size, -1),\n","         tf.constant([1, 1], dtype=\"int32\")], axis=0\n","    )\n","    return tf.tile(mask, mult)\n","\n","  def call(self, inputs, encoder_outputs, mask=None):\n","    causal_mask = self.get_causal_attention_mask(inputs)\n","    if mask is not None:\n","      padding_mask = tf.cast(\n","          mask[:, tf.newaxis, :], dtype=\"int32\"\n","      )\n","      padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","    attention_output_1 = self.attention_1(\n","        query=inputs,\n","        value=inputs,\n","        key=inputs,\n","        attention_mask = causal_mask\n","    )\n","    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","    attention_output_2 = self.attention_2(\n","        query=inputs,\n","        value=encoder_outputs,\n","        key=encoder_outputs,\n","        attention_mask = padding_mask\n","    )\n","    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n","    proj_output = self.dense_proj(attention_output_2)\n","    return self.layernorm_3(attention_output_2 + proj_output)\n","\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","        \"embed_dim\" : embed_dim,\n","        \"num_heads\" : num_heads,\n","        \"dense_dim\" : dense_dim\n","    })\n","    return config"],"metadata":{"id":"rl6t-65gPd70","executionInfo":{"status":"ok","timestamp":1695280096211,"user_tz":-540,"elapsed":2,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"metadata":{"id":"BV4EPsNcUqzu","executionInfo":{"status":"ok","timestamp":1695280100318,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준일","userId":"10460237312093264831"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint(\"eng_to_spa_by_first_transformer.x\",\n","                                       save_best_only=True),\n","    keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5)\n","]\n","\n","transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=20, callbacks=callbacks, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFAWSZ0NVbRj","executionInfo":{"status":"ok","timestamp":1695282186550,"user_tz":-540,"elapsed":2084903,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"c5357008-0e82-45e1-baf8-14808a1b7537"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1302/1302 [==============================] - 113s 82ms/step - loss: 1.5440 - accuracy: 0.7832 - val_loss: 1.2034 - val_accuracy: 0.8106 - lr: 0.0010\n","Epoch 2/20\n","1302/1302 [==============================] - 93s 71ms/step - loss: 1.1936 - accuracy: 0.8136 - val_loss: 1.0523 - val_accuracy: 0.8288 - lr: 0.0010\n","Epoch 3/20\n","1302/1302 [==============================] - 93s 71ms/step - loss: 1.0743 - accuracy: 0.8283 - val_loss: 0.9765 - val_accuracy: 0.8407 - lr: 0.0010\n","Epoch 4/20\n","1302/1302 [==============================] - 93s 71ms/step - loss: 0.9995 - accuracy: 0.8390 - val_loss: 0.9306 - val_accuracy: 0.8484 - lr: 0.0010\n","Epoch 5/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.9465 - accuracy: 0.8467 - val_loss: 0.9078 - val_accuracy: 0.8516 - lr: 0.0010\n","Epoch 6/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.9057 - accuracy: 0.8534 - val_loss: 0.8889 - val_accuracy: 0.8551 - lr: 0.0010\n","Epoch 7/20\n","1302/1302 [==============================] - 94s 72ms/step - loss: 0.8716 - accuracy: 0.8590 - val_loss: 0.8774 - val_accuracy: 0.8583 - lr: 0.0010\n","Epoch 8/20\n","1302/1302 [==============================] - 93s 71ms/step - loss: 0.8301 - accuracy: 0.8660 - val_loss: 0.8371 - val_accuracy: 0.8655 - lr: 0.0010\n","Epoch 9/20\n","1302/1302 [==============================] - 94s 73ms/step - loss: 0.7941 - accuracy: 0.8724 - val_loss: 0.8158 - val_accuracy: 0.8704 - lr: 0.0010\n","Epoch 10/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.7662 - accuracy: 0.8770 - val_loss: 0.8037 - val_accuracy: 0.8729 - lr: 0.0010\n","Epoch 11/20\n","1302/1302 [==============================] - 93s 71ms/step - loss: 0.7429 - accuracy: 0.8810 - val_loss: 0.7988 - val_accuracy: 0.8737 - lr: 0.0010\n","Epoch 12/20\n","1302/1302 [==============================] - 94s 72ms/step - loss: 0.7244 - accuracy: 0.8842 - val_loss: 0.7920 - val_accuracy: 0.8756 - lr: 0.0010\n","Epoch 13/20\n","1302/1302 [==============================] - 88s 68ms/step - loss: 0.7082 - accuracy: 0.8870 - val_loss: 0.7974 - val_accuracy: 0.8747 - lr: 0.0010\n","Epoch 14/20\n","1302/1302 [==============================] - 88s 68ms/step - loss: 0.6956 - accuracy: 0.8893 - val_loss: 0.7953 - val_accuracy: 0.8759 - lr: 0.0010\n","Epoch 15/20\n","1302/1302 [==============================] - 88s 68ms/step - loss: 0.6834 - accuracy: 0.8917 - val_loss: 0.7933 - val_accuracy: 0.8773 - lr: 0.0010\n","Epoch 16/20\n","1302/1302 [==============================] - 88s 68ms/step - loss: 0.6739 - accuracy: 0.8937 - val_loss: 0.7921 - val_accuracy: 0.8784 - lr: 0.0010\n","Epoch 17/20\n","1302/1302 [==============================] - 88s 67ms/step - loss: 0.6640 - accuracy: 0.8953 - val_loss: 0.8070 - val_accuracy: 0.8771 - lr: 0.0010\n","Epoch 18/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.5470 - accuracy: 0.9138 - val_loss: 0.7076 - val_accuracy: 0.8928 - lr: 1.0000e-04\n","Epoch 19/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.5058 - accuracy: 0.9210 - val_loss: 0.7017 - val_accuracy: 0.8935 - lr: 1.0000e-04\n","Epoch 20/20\n","1302/1302 [==============================] - 93s 72ms/step - loss: 0.4897 - accuracy: 0.9237 - val_loss: 0.6986 - val_accuracy: 0.8940 - lr: 1.0000e-04\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a77c839bc70>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import numpy as np\n","\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","  tokenized_input_sentence = source_vectorization([input_sentence])\n","  decoded_sentence = \"[start]\"\n","  for i in range(max_decoded_sentence_length):\n","    tokenized_target_sentence = target_vectorization(\n","        [decoded_sentence])[:, :-1]\n","    predictions = transformer(\n","        [tokenized_input_sentence, tokenized_target_sentence]\n","    )\n","    sampled_token_index = np.argmax(predictions[0, i, :])\n","    sampled_token = spa_index_lookup[sampled_token_index]\n","    decoded_sentence += \" \" + sampled_token\n","    if sampled_token == \"[end]\":\n","      break\n","  return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(10):\n","  input_sentence = random.choice(test_eng_texts)\n","  print(\"-\")\n","  print(input_sentence)\n","  print(decode_sequence(input_sentence))\n","  print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xERf0sgdB36","executionInfo":{"status":"ok","timestamp":1695282896992,"user_tz":-540,"elapsed":12434,"user":{"displayName":"박준일","userId":"10460237312093264831"}},"outputId":"8ad096b9-0ffb-4e05-ee88-382892112b41"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","You're afraid of him.\n","[start] miedo[end]                   \n","\n","\n","-\n","I could kill you.\n","[start] podría matar[end]                  \n","\n","\n","-\n","My grandpa drinks coffee with a group of old guys every morning.\n","[start] el café con un [UNK] de [UNK] de todas las mañanas[end]         \n","\n","\n","-\n","I can't wait for spring to come so we can sit under the cherry trees.\n","[start] no puedo llegar a la primavera hasta los [UNK] cerca de los árboles[end]       \n","\n","\n","-\n","That never happens around here.\n","[start] aquí no hay de eso[end]               \n","\n","\n","-\n","He got in with a shotgun in his hands.\n","[start] con una [UNK] en la mano[end]              \n","\n","\n","-\n","I sometimes lie on the grass.\n","[start] a veces en la [UNK]               \n","\n","\n","-\n","The temperature is above average this winter.\n","[start] la temperatura está [UNK] en invierno[end]              \n","\n","\n","-\n","What rotten luck!\n","[start] qué suerte[end]                  \n","\n","\n","-\n","I couldn't make myself heard in the noisy class.\n","[start] no pude leer a lo suficientemente [UNK]             \n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HzrKthXat7cX"},"execution_count":null,"outputs":[]}]}